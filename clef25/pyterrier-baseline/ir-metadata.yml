# We use the ir-metadata standard to describe and process runs.
# The fields below are mandatory, you can add additional metadata if you like.
# There are two libraries that automate and simplify tracking of experimental metadata that you can use optionally:
#
#   - The metadata module of [repro_eval](https://www.ir-metadata.org/software/):
#     Tracks the platform and implementation of your experiments.
#
#   - The [tirex_tracker](https://github.com/tira-io/tirex-tracker/):
#     Tracks the platform, implementation, and resources (e.g., CPU, GPU, RAM, emissions, etc.) of your experiments.
#
# See https://www.ir-metadata.org for more details on how  on fields that you can incorporate

tag: ENTER_VALUE_HERE

actor:
  # The name of the team
  team: ENTER_VALUE_HERE

# Please provide a short description of your system.
description: |
  A PyTerrier baseline that uses the ir-datasets integration for LongEval.
  
  We use BM25 and leave everything to the defaults of PyTerrier.
  I.e., no training data is used at all.

platform:
  software:
    # Which software and tools did you use for training, tunning and running your system?
    # You can maintain the software that you used manually.
    # Alternatively, you can use repro_eval or the tirex_tracker to track this.
    libraries:
      - PyTerrier 0.10.0

implementation:
  source:
    # Please provide a reference to your code if possible.
    # If you can not share your code, you can delete the implementation section.
    # The repro_eval and/or tirex_tracker tools can track this automatically, including commits, branches, etc.
    repository: https://github.com/OpenWebSearch/LONGEVAL-24

data:
  # Please describe which training data your system used, e.g., longeval-sci, longeval-web, MS MARCO, etc.
  training data:
    - name: Nothing

method:
  # Boolean value indicating if it is a automatic (true) or manual (false) run
  automatic: true

  indexing:
    tokenizer: Default from PyTerrier
    stemmer: Default from PyTerrier
    stopwords: Default from PyTerrier

  retrieval:
    - # Which ranking approach do you use? E.g., bm25
      name: Key Queries

      ##################################################
      # Yes/No Questions
      ##################################################

      # Did you use any statistical ranking model? (yes/no)
      lexical: yes

      # Did you use any deep neural network model? (yes/no)
      deep_neural_model: no

      # Did you use a sparse neural model? (yes/no):
      sparse_neural_model: no

      # Did you use a dense neural model? (yes/no):
      dense_neural_model: no

      # Did you use more than a single retrieval model? (yes/no):
      single_stage_retrieval: no
